{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Unsupervised Learning: Clustering Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. Initial practice with the K-means and HAC algorithms\n",
        "\n",
        "### 1.1 (10%) K-means\n",
        "Run K-means on this [Abalone Dataset.](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
        "The dataset was modified to be smaller. The last datapoint should be on line 359 or the point 0.585,0.46,0.185,0.922,0.3635,0.213,0.285,10. The remaining points are commented out. Treat the output class (last column) as an additional input feature. Create your K-Mmeans model with the paramaters K-means(n_clusters=3, init='random', n_init=1) \n",
        "\n",
        "Output the following:\n",
        "- Class label for each point (labels_)\n",
        "- The k=3 cluster centers (cluster_centers_)\n",
        "- Number of iterations it took to converge (n_iter_)\n",
        "- Total sum squared error of each point from its cluster center (inertia_)\n",
        "- The total average silhouette score (see sklearn.metrics silhouette_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The array of the class label for each point is: \n",
            "[0 1 1 1 1 1 0 0 1 0 2 1 2 1 1 2 1 1 1 1 2 1 2 1 1 2 2 2 0 2 1 0 0 0 2 1 0\n",
            " 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 1 0 2\n",
            " 2 0 1 1 2 2 1 2 0 0 2 2 2 1 1 2 0 2 2 1 0 2 1 1 1 1 1 0 0 1 2 2 2 1 1 1 1\n",
            " 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 2 1 1 1 1\n",
            " 1 1 0 1 1 1 2 1 2 0 2 2 2 2 2 0 0 2 0 0 2 2 2 1 1 1 1 1 1 1 1 2 2 0 1 1 2\n",
            " 2 2 1 2 1 2 2 2 1 1 2 2 0 0 1]\n",
            "\n",
            "The centers of each cluster are as shown: \n",
            "[[ 0.61366667  0.48933333  0.16716667  1.29283333  0.48815     0.25873333\n",
            "   0.45488333 17.06666667]\n",
            " [ 0.44221239  0.3439823   0.11331858  0.49762832  0.20678319  0.11430973\n",
            "   0.15534513  8.24778761]\n",
            " [ 0.58122807  0.45649123  0.1604386   1.05523684  0.41045614  0.231\n",
            "   0.33986842 12.33333333]]\n",
            "\n",
            "It took 4 iterations to converge.\n",
            "The total sum squared error of each point from its cluster center is  540.2109608826968\n",
            "The total average silhouette score is  0.5010728634549578\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# read in dataset\n",
        "# convert dataset to pd\n",
        "# split data into X and y\n",
        "# normalize data?\n",
        "# declare model\n",
        "# train_test_split\n",
        "# fit\n",
        "# analyze\n",
        "\n",
        "abalone_data = arff.loadarff('abalone.arff')\n",
        "abalone_df = pd.DataFrame(abalone_data[0])\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(abalone_df)\n",
        "\n",
        "# K-means with Abalone\n",
        "model = KMeans(n_clusters=3, init='random', n_init=1)\n",
        "model.fit(abalone_df)\n",
        "print(\"The array of the class label for each point is: \")\n",
        "print(model.labels_)\n",
        "print(\"\\nThe centers of each cluster are as shown: \")\n",
        "print(model.cluster_centers_)\n",
        "print(\"\\nIt took \" + str(model.n_iter_) + \" iterations to converge.\")\n",
        "print(\"The total sum squared error of each point from its cluster center is \", model.inertia_)\n",
        "print(\"The total average silhouette score is \", silhouette_score(abalone_df, model.labels_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion*: The three clusters look to have different population sizes, with the '0' cluster being the smallest, and the '2' cluster holding the most datapoints. It looks like cluster '0' and cluster '2' have centroids that are fairly close together, while cluster '1' has a centroid that is a little further away. The model only took 5 iterations to converge, which was a lot fewer than I expected. I suspect this is because we are working with a smaller dataset and perhaps the clusters are fairly distinct. However, the sum squared error and the silhouette score work against this hypothesis, as the model only had a score of .51, which is a pretty bad silhouette score. This means that there were likely points that the K-Means algorithm labeled as a certain cluster, that really should have been assigned to another cluster. If I were guessing, I would say that the silhouette score would improve if the K-Means model was trained for a different number of centroids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "### 1.2 (10%) Hierarchical Agglomerative Clustering (HAC) \n",
        "\n",
        "Run HAC on the same [Abalone Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff) using complete linkage and k=3.\n",
        "\n",
        "Output the following:\n",
        "- Class label for each point (labels_)\n",
        "- The total average silhouette score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The array of the class label for each point is: \n",
            "[1 0 0 0 0 0 2 1 0 2 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 2 2 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 2 1\n",
            " 1 1 0 0 1 1 0 1 1 2 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 2 1 1 2 2 1 1 1 0 0 0 0 0 0 0 0 1 1 2 0 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0]\n",
            "The total average silhouette score is  0.5398112398376158\n"
          ]
        }
      ],
      "source": [
        "# HAC with Abalone\n",
        "model = AgglomerativeClustering(linkage='complete', n_clusters=3)\n",
        "model.fit(abalone_df)\n",
        "print(\"The array of the class label for each point is: \")\n",
        "print(model.labels_)\n",
        "print(\"The total average silhouette score is \",\n",
        "      silhouette_score(abalone_df, model.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion*: The HAC model clustered "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. K-means Clustering with the [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "Use the Iris data set for 2.1 and 2.2.  Don't include the output label as one of the input features.\n",
        "\n",
        "### 2.1 (20%) K-means Initial Centroids Experiments\n",
        "K-means results differ based on the initial centroids used.\n",
        "- Run K-means 5 times with *k*=4, each time with different initial random centroids (init=\"random) and with n_init=1.  Give inertia and silhouette scores for each run and discuss any variations in the results.\n",
        "- SKlearn has a parameter that does this automatically (n_init).  n_init = z runs K-means z times, each with different random centroids and returns the clustering with the best SSE (intertia) of the z runs. Try it out and discuss how it does and how it compares with your 5 runs above.\n",
        "- Sklearn also has a parameter (init:'K-means++') which runs a simpler fast version of K-means first on the data to come up with good initial centroids, and then runs regular K-means with this centroids.  Try it out (with n_init = 1) and discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "outputs": [],
      "source": [
        "# K-means initial centroid experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjkatnQY-Jep"
      },
      "source": [
        "Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGfrL7p5-Jeq"
      },
      "source": [
        "### 2.2 (20%) Silhouette Graphs\n",
        "In this part you will show silhouette graphs for different *k* values.  Install the [Yellowbrick visualization package](https://www.scikit-yb.org/en/latest/quickstart.html) and import the [Silhouette Visualizer](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html).  This library includes lots of visualization packages which you might find useful. (Note: The YellowBrick silhouette visualizer does not currently support HAC).\n",
        "- Show Silhouette graphs for clusterings with *k* = 2-6. Print the SSE (inertia) and total silhouette score for each.\n",
        "- Learn with the default n_init = 10 to help insure a decent clustering.\n",
        "- Using the silhouette graphs, choose which *k* you think is best and discuss why. Think about and discuss more than just the total silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gigfanaW-Jeq"
      },
      "outputs": [],
      "source": [
        "# Iris Clustering with K-means and silhouette graphs\n",
        "from yellowbrick.cluster import SilhouetteVisualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IELq_zlu-Jeq"
      },
      "source": [
        "Discuss your results and justify which clustering is best based on the silhouette graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 (20%) Iris Clustering with HAC\n",
        "\n",
        "- Use the same dataset as above and learn with HAC clustering\n",
        "- Create one table with silhouette scores for k=2-6 for each of the linkage options single, average, complete, and ward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HAC with Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion and linkage comparison*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 4 (20%) Run both algorithms on a real world data\n",
        "- Choose any real world data set which you have not used previously\n",
        "- Use parameters of your choosing\n",
        "- Try each algorithm a few times with different parameters and output one typical example of labels and silhouette scores for each algorithm\n",
        "- Show the silhouette graph for at least one reasonable *k* value for K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "outputs": [],
      "source": [
        "# Run both algoriths on a data set of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-oKHPPT-Jer"
      },
      "source": [
        "*Discussion and comparison*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extra Credit for Coding Your Own Clustering Algorithms\n",
        "### 5.1 (Optional 10% extra credit) Code up the K-means clustering algorithm \n",
        "Below is a scaffold you could use if you want. As above, you only need to support numeric inputs, but think about how you would support nominal inputs and unknown values. Requirements for this task:\n",
        "- Your model should support the methods shown in the example scaffold below.\n",
        "- Ability to choose *k* and specify the *k* initial centroids.\n",
        "- Run and show the cluster label for each point with both the Iris data set and the data set of your choice above.\n",
        "\n",
        "### 5.2 (Optional 10% extra credit) Code up the HAC clustering algorithm \n",
        "\n",
        "- Your model should support the methods shown in the example scaffold below.\n",
        "- HAC should support both single link and complete link options.\n",
        "- HAC automatically generates all clusterings from *n* to 2.  You just need to output results for the curent chosen *k*.\n",
        "- Run and show the cluster label for each point with both the Iris data set and the data set of your choice above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion and comparision of each model implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin, ClusterMixin\n",
        "\n",
        "class KMEANSClustering(BaseEstimator,ClusterMixin):\n",
        "\n",
        "    def __init__(self,k=3,debug=False): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            debug = if debug is true use the first k instances as the initial centroids otherwise choose random points as the initial centroids.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.debug = debug\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        return self\n",
        "    \n",
        "    def print_labels(self): # Print the cluster label for each data point\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HACClustering(BaseEstimator,ClusterMixin):\n",
        "\n",
        "    def __init__(self,k=3,link_type='single'): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            link_type = single or complete. when combining two clusters use complete link or single link\n",
        "        \"\"\"\n",
        "        self.link_type = link_type\n",
        "        self.k = k\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        return self\n",
        "    \n",
        "    def print_labels(self): # Print the cluster label for each data point\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
